# -*- coding: utf-8 -*-
"""offensiveanalysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/gist/ZeppLayy/6dcef9f5c9d4cb206acd15f280a93a73/offensiveanalysis.ipynb

Khai báo thư viện:
"""

import tensorflow as tf
from tensorflow import keras

import string
import os
import tempfile
import operator
from textblob import TextBlob

import matplotlib as mpl
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import warnings

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, GRU,Embedding,Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.text import Tokenizer


from sklearn.feature_extraction.text import CountVectorizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Dense, Activation, Embedding, LSTM, SpatialDropout1D
from keras.layers import LSTM, Conv1D, MaxPooling1D
from sklearn.model_selection import train_test_split
from keras.utils.np_utils import to_categorical
from sklearn.utils import resample
from sklearn.utils import shuffle
from sklearn.metrics import confusion_matrix,classification_report

import sklearn
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

#For Preprocessing
import re    # RegEx for removing non-letter characters
import nltk  #natural language processing
nltk.download("stopwords")
from nltk.corpus import stopwords
from nltk.stem.porter import *
import nltk
nltk.download('punkt')

pd.set_option("display.max_colwidth", 200) 
warnings.filterwarnings("ignore", category=DeprecationWarning)

"""Tải dữ liệu:"""

from google.colab import files
uploaded = files.upload()

from google.colab import files
uploaded = files.upload()

"""Xử lý dữ liệu:"""

df = pd.read_csv("Twitter.csv")

df2 = pd.read_csv("labeled_data.csv")
df2 = df2[['tweet','class']]
df2 = df2.rename(columns={'tweet': 'clean_text', 'class':'label'})
df2['label'] = df2['label'].replace([0,1,2],[1,1,0])

test1 = df.dropna()

test2 = df2.dropna()

##1.Xử lý dữ liệu:
pd.set_option('mode.chained_assignment', None)

def remove_pattern(input_txt, pattern):
    r = re.findall(pattern, input_txt)
    for i in r:
        input_txt = re.sub(i, '', input_txt)
    return input_txt

test1['try'] = np.vectorize(remove_pattern)(test1['clean_text'], "@[\w]*") 
## Xóa dấu chấm câu
test1['try'] = test1['try'].str.replace('[^\w\s]',' ')
## Xóa khoảng trắng giữa các từ
test1['try'] = test1['try'].str.replace(' +', ' ')
## Xóa số
test1['try'] = test1['try'].str.replace('\d+', '')
## Xóa dấu cách ở cuối
test1['try'] = test1['try'].str.strip()
## Xóa URL
test1['try'] = test1['try'].apply(lambda x: re.split('https:\/\/.*', str(x))[0])
## Xóa các từ dừng
stop = stopwords.words('english')
test1['try'] = test1['try'].apply(lambda x: " ".join(x for x in x.split() if x not in stop ))

test2['try'] = np.vectorize(remove_pattern)(test2['clean_text'], "@[\w]*") 

test2['try'] = test2['try'].str.replace('[^\w\s]',' ')

test2['try'] = test2['try'].str.replace('RT',' ')

test2['try'] = test2['try'].apply(lambda x: x.lower())

test2['try'] = test2['try'].str.replace(' +', ' ')

test2['try'] = test2['try'].str.replace('\d+', '')

test2['try'] = test2['try'].str.strip()

test2['try'] = test2['try'].apply(lambda x: re.split('https:\/\/.*', str(x))[0])

stop = stopwords.words('english')
test2['try'] = test2['try'].apply(lambda x: " ".join(x for x in x.split() if x not in stop ))

tokenized_tweet = test1['try'].apply(lambda x: x.split()) # tokenizing 
tokenized_tweet.head()

tokenized_tweet2 = test2['try'].apply(lambda x: x.split()) # tokenizing 
tokenized_tweet2.head()

from nltk.stem.porter import * 
stemmer = PorterStemmer() 
# stemming
max_word = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x])

for i in range(len(tokenized_tweet)):
   tokenized_tweet[i]  = ' '.join(map(str, tokenized_tweet[i]))
test1['try'] = tokenized_tweet

for i in range(len(max_word)):
   max_word[i]  = ' '.join(map(str, max_word[i]))
max_words = max_word

words = ' '.join(test1['try'])
words = words.split()

max_words = ' '.join(max_words)
max_words = max_words.split()

len(max_words)

"""Tiến hành gắn nhãn các tweet có nội dung phản cảm:"""

from google.colab import files
uploaded = files.upload()

bad = []
with open('final_wordz.txt') as f:
  bad = f.readlines()

len(bad)

def count_word(badword):
  badword2= dict()
#  Get frequency for each words where word is the key and the count is the value
  for word in (badword):
    word = word.lower()
    badword2[word] = badword2.get(word, 0) + 1
  return badword2

badword = []
for i in range(len(bad)):
  word = bad[i].replace('\n', '').replace("'", '').replace(",",'').strip()
  badword.append(word)

z = list(dict(sorted(count_word(badword).items(), key=operator.itemgetter(1),reverse=True)))

z

len(z)

c = list(dict(sorted(count_word(max_words).items(), key=operator.itemgetter(1),reverse=True)))

len(c)

x = []
for i in range(len(words)):
  if words[i] in z:
    x.append(words[i])

x

len(x)

x = (sorted(count_word(x).items(),key=lambda x: x[1] ,reverse=True))

len(x)

tokenz = test1['try'].apply(lambda x: x.split()) # tokenizing 
tokenz.head()

test1['label'] = 0

for i in range(len(tokenz)):
    for j in range(len(tokenz[i])):
          if tokenz[i][j] in z:
                test1['label'][i] = 1

test1['label'].value_counts()

test1[test1['label'] == 1]

data_1 = test1[['try','label']]
data_2 = test2[['try','label']]

finaldata = pd.concat([data_1, data_2], ignore_index=True)

finaldata.tail()

data_p = finaldata[finaldata['label'] == 0]
data_n = finaldata[finaldata['label'] == 1]

data_p.shape

data_n.shape

datap = data_p.iloc[np.random.randint(1,data_p.shape[0],50000), :]
datan = data_n.iloc[np.random.randint(1,data_n.shape[0],50000), :]
len(datan), len(datap)

data = pd.concat([datap,datan])
len(data)

sns.countplot(data['label'])
plt.show()

X_text=data['try']
y=data['label']

"""Xây dựng mô hình phân loại nhị phân với Neutral Network:"""

X_train1,X_test1,y_train1,y_test1=train_test_split(X_text,y,test_size=0.01,random_state=0)
X_train_text,X_test_text,y_train,y_test=train_test_split(X_train1,y_train1,test_size=0.3,random_state=0)

num_words = 3000
Tokenizer = Tokenizer(num_words=num_words)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# Tokenizer.fit_on_texts(X_text)

X_train_Tokens = Tokenizer.texts_to_sequences(X_train_text)
X_test_tokens = Tokenizer.texts_to_sequences(X_test_text)

num_tokens = [len(tokens) for tokens in X_train_Tokens + X_test_tokens]
num_tokens = np.array(num_tokens)

max_tokens = np.mean(num_tokens) + 2 *np.std(num_tokens)
max_tokens = int(max_tokens)
max_tokens

pad = 'pre'

X_train_pad = pad_sequences(X_train_Tokens, maxlen=max_tokens, padding=pad, truncating=pad)
X_test_pad = pad_sequences(X_test_tokens, maxlen=max_tokens, padding=pad, truncating=pad)

model = Sequential()
embedding_size = 28
model.add(Embedding(input_dim=num_words,
                    output_dim=embedding_size,
                    input_length=max_tokens,
                    name='layer_embedding'))
model.add(GRU(units=8, dropout=0.5, return_sequences=True))
model.add(GRU(units=4, return_sequences=True))
model.add(GRU(units=1,dropout=0.5,))
model.add(Dense(1, activation='sigmoid'))

optimizer = Adam(lr=1e-3)
model.compile(loss='binary_crossentropy', 
            optimizer=optimizer,
              metrics=['accuracy'])
model.summary()

history = model.fit(X_train_pad, y_train,
          validation_split=0.05, epochs=7, batch_size=16)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# result = model.evaluate(X_test_pad, y_test)

"""Dự đoán với mô hình neural network:

Chuẩn bị dữ liệu:
"""

tweet1 = "you should all sit down together and watch the simpsons episode where lisa becomes buddhist simpsons season episode she little faith then discuss "
tweet2 = "does evil include the lady pai chunked "
tweet3 = "blood and souls for lord arioch "
tweet4 = "dont worry about trying explain yourself just meditate regularly and try hard you can more aware everything else will follow coming from someone who has been throught his situation welcome pms "
tweet5 = "This name shall lump wherever lumps are there warts tree burls breasts humpback whales "
tweet6 = "demogorgon because fuck you and your shit god "
tweet7 = "Not a good movie!"
tweet8 = "exactly the song was jamming when made this "
tweet9 = "talk all the nonsense and continue all the drama will vote for modi"
tweet10 = "gay this comes from cabinet which has scholars like modi smriti and hema time introspect"

datas = pd.DataFrame()
datas['text'] = [tweet1, tweet2, tweet3, tweet4, tweet5, tweet6, tweet7, tweet8, tweet9, tweet10]

texts = datas['text'].apply(lambda x: x.split())
from nltk.stem.porter import * 
stemmer = PorterStemmer() 
texts = texts.apply(lambda x: [stemmer.stem(i) for i in x])
for i in range(len(texts)):
   texts[i]  = ' '.join(map(str, texts[i]))

tokens = Tokenizer.texts_to_sequences(texts)

tokens_pad = pad_sequences(tokens, maxlen=max_tokens)
tokens_pad.shape

predict = model.predict(np.array(tokens_pad))
for i in range(len(predict)):
  print('Predict',i +1 ,':')
  if ((predict[i]) > 0.5):
      print('Sentiment =', predict[i], 'Offensive\n')

  else:
      print('Sentiment =', predict[i], 'not Offensive\n')

def plot_training_hist(history):
    '''Function to plot history for accuracy and loss'''
    
    fig, ax = plt.subplots(1, 2, figsize=(10,4))
    # first plot
    ax[0].plot(history.history['accuracy'])
    ax[0].plot(history.history['val_accuracy'])
    ax[0].set_title('Model Accuracy')
    ax[0].set_xlabel('epoch')
    ax[0].set_ylabel('accuracy')
    ax[0].legend(['train', 'validation'], loc='best')
    # second plot
    ax[1].plot(history.history['loss'])
    ax[1].plot(history.history['val_loss'])
    ax[1].set_title('Model Loss')
    ax[1].set_xlabel('epoch')
    ax[1].set_ylabel('loss')
    ax[1].legend(['train', 'validation'], loc='best')
    
plot_training_hist(history)

"""Chạy thử với các mô hình nhị phân khác để so sánh độ chính xác:

LogisticRegression:
"""

from sklearn.linear_model import LogisticRegression

LR = LogisticRegression(solver='lbfgs', max_iter=1000)
LR.fit(X_train_pad, y_train)
print("Train Score:",LR.score(X_train_pad, y_train))
print("Test Score:",LR.score(X_test_pad, y_test))

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, f1_score

cm = confusion_matrix(y_test, LR.predict(X_test_pad))
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Reds)
plt.show()

print(classification_report(y_test, LR.predict(X_test_pad)))

"""RandomForestClassifier:

"""

from sklearn.ensemble import RandomForestClassifier

RF = RandomForestClassifier()
RF.fit(X_train_pad, y_train)
print("Train Score:",RF.score(X_train_pad, y_train))
print("Test Score:",RF.score(X_test_pad, y_test))

cm = confusion_matrix(y_test, RF.predict(X_test_pad))
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Greens)
plt.show()

print(classification_report(y_test, RF.predict(X_test_pad)))

"""Naive Bayes là một thuật toán phân loại cho các vấn đề phân loại nhị phân (hai lớp) và đa lớp. Kỹ thuật này dễ hiểu nhất khi được mô tả bằng các giá trị đầu vào nhị phân hoặc phân loại. Một số mô hình Naive Bayes:

BernoulliNB:
"""

from sklearn.naive_bayes import BernoulliNB 
BNB = BernoulliNB()
BNB.fit(X_train_pad, y_train)
print("Train Score:",BNB.score(X_train_pad, y_train))
print("Test Score:",BNB.score(X_test_pad, y_test))

cm = confusion_matrix(y_test, BNB.predict(X_test_pad))
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Greys)
plt.show()

print(classification_report(y_test, BNB.predict(X_test_pad)))

"""KNeighborsClassifier:"""

from sklearn.neighbors import KNeighborsClassifier
KN = KNeighborsClassifier()
KN.fit(X_train_pad, y_train)
print("Train Score:",KN.score(X_train_pad, y_train))
print("Test Score:",KN.score(X_test_pad, y_test))

cm = confusion_matrix(y_test, KN.predict(X_test_pad))
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.cividis)
plt.show()

print(classification_report(y_test, KN.predict(X_test_pad)))

"""DecisionTreeClassifier:"""

from sklearn.tree import DecisionTreeClassifier
DC = DecisionTreeClassifier()
DC.fit(X_train_pad, y_train)
print("Train Score:",DC.score(X_train_pad, y_train))
print("Test Score:",DC.score(X_test_pad, y_test))

cm = confusion_matrix(y_test, DC.predict(X_test_pad))
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.inferno)
plt.show()

print(classification_report(y_test, DC.predict(X_test_pad)))

"""MLPClassifier:"""

from sklearn.neural_network import MLPClassifier
MLP = DecisionTreeClassifier()
MLP.fit(X_train_pad, y_train)
print("Train Score:",MLP.score(X_train_pad, y_train))
print("Test Score:",MLP.score(X_test_pad, y_test))

cm = confusion_matrix(y_test, MLP.predict(X_test_pad))
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.plasma)
plt.show()

print(classification_report(y_test, MLP.predict(X_test_pad)))

"""AdaBoostClassifier:"""

from sklearn.ensemble import AdaBoostClassifier
ABC = DecisionTreeClassifier()
ABC.fit(X_train_pad, y_train)
print("Train Score:",ABC.score(X_train_pad, y_train))
print("Test Score:",ABC.score(X_test_pad, y_test))

cm = confusion_matrix(y_test, ABC.predict(X_test_pad))
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.viridis)
plt.show()

print(classification_report(y_test, ABC.predict(X_test_pad)))

"""XGBClassifier:"""

from xgboost import XGBClassifier
XGB = XGBClassifier()
XGB.fit(X_train_pad, y_train)
print("Train Score:",XGB.score(X_train_pad, y_train))
print("Test Score:",XGB.score(X_test_pad, y_test))

cm = confusion_matrix(y_test, XGB.predict(X_test_pad))
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.GnBu)
plt.show()

print(classification_report(y_test, XGB.predict(X_test_pad)))

"""SVC:"""

from sklearn.svm import SVC
SVM = SVC()
SVM.fit(X_train_pad, y_train)
print("Train Score:",SVM.score(X_train_pad, y_train))
print("Test Score:",SVM.score(X_test_pad, y_test))

cm = confusion_matrix(y_test, SVM.predict(X_test_pad))
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.YlOrRd)
plt.show()

print(classification_report(y_test, SVM.predict(X_test_pad)))

"""So sánh hiệu quả của các mô hình:

Sử dụng chỉ số accuracy để so sánh:
1. Neural Network = 96%
2. Logistic Regression = 54%
3. Random Forest Classifier = 88%
4. BernoulliNB = 55%
5. KNeighborsClassifier = 62%
6. DecisionTreeClassifier = 83%
7. MLPClassifier = 83%
8. AdaBoostClassifier = 83%
9. XGBClassifier = 73%
10. Support Vector Machine = 58%
"""

